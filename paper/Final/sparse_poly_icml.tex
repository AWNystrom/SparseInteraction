%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2017 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2017,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amssymb}
\usepackage{bm}
\usepackage{clrscode3e}
\usepackage{float}
\usepackage{footmisc}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2017} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2017}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Leveraging Matrix Sparsity in Polynomial Expansions}

\begin{document} 

\twocolumn[
\icmltitle{Leveraging Matrix Sparsity in Polynomial Expansions}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% you can specify symbols, otherwise they are numbered in order
% ideally, you should not use this facility. affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo} 
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract}
We provide an algorithm for polynomial feature expansion that both operates on and produces a compressed sparse row matrix without any densification.
For a vector of dimension $D$, density $d$, and degree $k$ the algorithm has time complexity $O(d^kD^k)$ where $k$ is the polynomial-feature order; this  is an improvement by a factor $d^k$ over the standard method.
\end{abstract}

\section{Introduction}

Feature mappings are an intra-instance transformation, usually referred to as $\phi(\vec{x})$, that map instance vectors to higher dimensional spaces in which they are more linearly separable, allowing linear models to capture non-linearities \cite{yuan2012recent}.
A widely used feature mapping is the polynomial expansion, which generates a new feature for each product of all $k$-order combinations in the original feature space.
A $k$-order polynomial expansion of the feature space allows a linear model to learn polynomial relationships between dependent and independant variables.
This mapping was first utilized in a published experiment by Joseph Diaz Gergonne in 1815 \cite{gergonne1974application, smith1918standard}.

While other methods for capturing nonlinearities have been developed, such as kernels (the direct offspring of feature mappings), trees, generative models, and neural networks, feature mappings are still a popular tool \cite{barker200114, chang2010training, shaw2006intellectual}.
The instance-independant nature of feature mappings allows them to pair well with linear parameter estimation techniques such as stochastic gradient descent, making them a candidate for certain types of large-scale machine learning problems when $D < N$.

The compressed sparse row (CSR) matrix format \cite(saad1994sparskit) is widely used \cite{liu2012sparse, bulucc2009parallel, bell2008efficient, white1997improving} and supported \cite{eigenweb, bastien2012theano, koenker2003sparsem}, and is considered the standard data structure for sparse, computationally heavy tasks.
However, polynomial expansions cannot be performed directly on sparse CSR matrices, nor on any sparse matrix format, without intermediate densification steps.
This densification not only adds extra overhead, but wastefully computes combinations of features that have a product of zero, which are then discarded during conversion into a sparse format.

We provide an algorithm that allows CSR matrices to be the input of a polynomial feature expansion without any densification.
The algorithm leverages the CSR format to only compute products of features that result in nonzero values.
This exploits the sparsity of the data to achieve an improved time complexity of $O(d^kD^k)$ on each vector of the matrix where $k$ is the degree of the expansion, $D$ is the dimensionality, and $d$ is the density.
The standard algorithm has time complexity $O(D^k)$.
Since $0 \le d \le 1$, our algorithm is a significant improvement.
While the algorithm we describe uses CSR matrices, it could be modified to operate on other sparse formats.

\section{Preliminaries}
Matrices are denoted by uppercase bold letters thus: $\bm{A}$. 
The $i$the row of $\bm{A}$ is written $\bm{a}_i$. All vectors are written in bold, and  $\bm{a}$, with no subscript, is a vector.

A compressed sparse row (CSR) matrix representation of an $r$-row matrix $\bm{A}$ consists of three vectors: $\bm{c}$, $\bm{d}$, and $\bm{p}$ and a single number: the number of columns of $\bm{A}$. The vectors
$\bm{c}$ and $\bm{d}$ contain the same number of elements, and hold the column indices and data values, respectively, of all nonzero elements of $\bm{A}$.
The vector $\bm{p}$ has $r$ entries. The values in $\bm{p}$ index both $\bm{c}$ and $\bm{d}$. The $i$th entry $\bm{p}_i$ of $\bm{p}$ tells where
the data describing nonzero columns of $\bm{a}_i$ are within the other two vectors: $\bm{c}_{\bm{p}_i:\bm{p}_{i+1}}$ contain the column indices of those entries; $\bm{d}_{\bm{p}_i:\bm{p}_{i+1}}$ contain the entries themselves.
Since only nonzero elements of each row are held, the overall number of columns of $\bm{A}$  must also be stored, since it cannot be derived
from the other data.

Scalars, vectors, and matrices are often referenced with the superscript $k$.
This is not to be interpreted as an exponent, but to indicate that it is the analogous aspect of that which procedes it, but in its polynomial expansion form.
For example, $\bm{c}^2$ is the vector that holds columns for nonzero values in $\bm{A}$'s quadratic feature expansion CSR representation.

For simplicity in the presentation, we work with polynomial expansions of degree 2, but continue to use the exponent $k$ to show how the ideas apply in the general case.

We do provide an algorithm for third degree expansions, and derive the big-O time complexity of the general case.

We have also developed an algorithm for second and third degree interaction features (combinations without repetition), which can be found in the implementation.

\section{Motivation}
In this section, we present a strawman algorithm for computing polynomial feature expansions on dense matrices.
We then modify the algorithm  slightly to operate on a CSR matrix, in order to expose its infeasibility in that context.
We then show how the algorithm would be feasible with an added component, which we then derive in the following section.

\subsection{Dense Expansion Algorithm}
A natural way to calculate polynomial features for a matrix $\bm{A}$ is to walk down its rows and, for each row, take products of all $k$-combinations of elements.
To determine in which column of $\bm{A}^k_i$ products of elements in $\bm{A}_i$ belong, a simple counter can be set to zero for each row of $\bm{A}$ and incremented efter each polynomial feature is generated.
This counter gives the column of $\bm{A}^k_i$ into which each expansion feature belongs.

\begin{codebox}
\footnotesize
\Procname{$\proc{Dense Second Order Algorithm}(\bm{A})$}
    \li $N \gets$ row count of $\bm{A}$
    \li $D \gets$ column count of $\bm{A}$
    \li $\bm{A}^k$ $\gets$ empty $N \times \binom{D}{2}$ matrix
    \li \For $i \gets 0 \To N-1$ \Do
    \li     $c_p \gets 0$
    \li     \For $j_1 \gets 0 \To D-1$ \Do
    \li         \For $j_2 \gets j_1 \To D-1$ \Do
    \li             $\bm{A}^k_{i{c_p}} \gets \bm{A}_{ij_1} \cdot \bm{A}_{ij_2}$
    \li             $c_p \gets c_p + 1$
                \End
            \End
       	\End
\end{codebox}

\subsection{Imperfect CSR Expansion Algorithm}
\label{sec:final-algo}
Now consider how this algorithm might be modified to accept a CSR matrix.
Instead of walking directly down rows of $\bm{A}$, we will walk down sections of $\bm{c}$ and $\bm{d}$ partitioned by $\bm{p}$, and instead of inserting polynomial features into $\bm{A}^k$, we will insert column numbers into $\bm{c}^k$ and data elements into $\bm{d}^k$.

\begin{codebox}
\footnotesize
\Procname{$\proc{Incomplete Sparse Second Order ALgorithm}(\bm{A})$}
    \li $N \gets$ row count of $\bm{A}$
    \li $\bm{p}^k$ $\gets$ vector of size $N+1$
    \li $\bm{p}^k_0 \gets 0$
    \li $nnz^k \gets 0$
    \li \For $i \gets 0 \To N-1$ \Do
    \li     $i_{start} \gets \bm{p}_i$
    \li     $i_{stop} \gets \bm{p}_{i+1}$
    \li     $\bm{c}_i \gets \bm{c}_{i_{start}:i_{stop}}$
    \li     $nnz^k_i \gets \binom{|\bm{c}_i|}{2}$ \label{li:row_nnz_count}
    \li     $nnz^k \gets nnz^k + nnz^k_i$
    \li     $\bm{p}^k_{i+1} \gets \bm{p}^k_i + nnz^k_i$
        \End
    \zi     
    \zi \Comment Build up the elements of $\bm{p}^k$, $\bm{c}^k$, and $\bm{d}^k$
    \li $\bm{p}^k$ $\gets$ vector of size $N+1$
    \li $\bm{c}^k$ $\gets$ vector of size $nnz^k$
    \li $\bm{d}^k$ $\gets$ vector of size $nnz^k$
    \li $n \gets 0$
    \li \For $i \gets 0 \To N-1$ \Do
    \li     $i_{start} \gets \bm{p}_i$
    \li     $i_{stop} \gets \bm{p}_{i+1}$
    \li     $\bm{c}_i \gets \bm{c}_{i_{start}:i_{stop}}$
    \li     $\bm{d}_i \gets \bm{d}_{i_{start}:i_{stop}}$
    \li     \For $c_1 \gets 0 \To |\bm{c}_i|-1$ \Do
    \li         \For $c_2 \gets c_1 \To |\bm{c}_i|-1$ \Do
%    \li             $j_0 \gets \bm{c}_{c_0}$
%    \li             $j_1 \gets \bm{c}_{c_1}$
    \li             $\bm{d}^k_{n} \gets \bm{d}_{c_0} \cdot \bm{d}_{c_1}$
    \li             $\bm{c}^k_{n} = ?$ \label{li:set_ck}
    \li             $n \gets n + 1$
                \End
            \End
       	\End
\end{codebox}

The crux of the problem is at line \ref{li:set_ck}.
Given the arbitrary columns involved in a polynomial feature of $\bm{A}_i$, we need to determine the corresponding column of $\bm{A}^k_i$.
We cannot simply reset a counter for each row as we did in the dense algorithm,  because only columns corresponding to nonzero values are stored.
Any time a column that would have held a zero value is implicitly skipped, the counter would err.

To develop a general algorithm, we require a mapping from columns of $\bm{A}$ to a column of $\bm{A}^k$.
If there are $D$ columns of $\bm{A}$ and $\binom{D}{k}$ columns of $\bm{A}^k$, this can be accomplished by a bijective mapping of the following form:

\begin{equation}
(j_0, j_1, \dots, j_{k-1}) \rightarrowtail \hspace{-1.9ex} \twoheadrightarrow p_{j_0j_1 \dots i_{k-1}} \in \{0,1,\dots,\binom{D}{k}-1\} 
\end{equation}

such that $ 0 \le j_0 \le j_1 \le \dots \le j_{k-1} < D$
where $(j_0, j_1, \dots, j_{k-1})$ are elements of $\bm{c}$ and $p_{j_0j_1 \dots i_{k-1}}$ is an element of $\bm{c}^k$. %column indicies of a row vector $\vec{x}$ of an $N \times D$ input matrix, and $p_{i_0i_1 \dots i_{k-1}}$ is a column index into the polynomial expansion vector for $\vec{x}$ where the product of elements corresponding to indices $i_0, i_1, \dots, i_{k-1}$ will be stored.

\section{Construction of Mapping}
Within this section, $i$, $j$, and $k$ denote column indices.
For the second degree case, we seek a map from matrix indices $(i, j)$ (with $0 \le i < j < D$ ) to numbers $f(i, j)$ with $0 \le f(i, j) < \frac{D(D-1)}{2}$, one that follows the pattern indicated by 
\begin{align}
\begin{bmatrix}
x & 0 & 1 & 3 \\
x & x & 2 & 4 \\
x & x & x & 5 \\
x & x & x & x
\end{bmatrix}
\label{eq:4x4mat}
\end{align}
where the entry in row $i$, column $j$, displays the value $f(i, j)$. We let $T_2(n) = \frac{1}{2} n(n+1)$ 
be the $n$th triangular number; then in Equation~\ref{eq:4x4mat}, column $j$ (for $j > 0$) contains entries with  
$T_2(j-1) \le e < T_2(j)$; the entry in the $i$th row is just $i + T_2(j-1)$. Thus we have
$
f(i, j) 
= i + T_2(j-1) =  \frac{1}{2}(2i + j^2-j).$
For instance, in column $j = 2$ in our example (the \emph{third} column), the entry in row $i = 1$ is 
$i + T_2(j-1) = 1 + 1 = 2$. 

With one-based indexing in both the domain and codomain, the formula above becomes
$f_1(i, j)  = \frac{1}{2}(2i + j^2 - 3j + 2).$

For \emph{polynomial} features, we seek a similar map $g$, one that also handles the case $i = j$. In this case, a similar analysis yields
$ g(i, j) = i + T_2(j) = \frac{1}{2} (2i + j^2 + j + 1).$


To handle \emph{three-way interactions}, we need to map triples of indices in a 3-index array to a flat list, and similarly for higher-order interactions. For this, we'll need the tetrahedral numbers $T_3(n) = \sum_{i=1}^n T_{2}(n) = 
\frac{1}{6}(n^3 + 3n^2 + 2n)$.

For three indices, $i,j,k$, with $0 \le i < j < k < D$, we have a similar recurrence. Calling the mapping $h$, we have 
\begin{align}
h(i,j,k) = i + T_2(j-1) + T_3(k-2);
\end{align}
if we define $T_1(i) = i$, then this has the very regular form
\begin{align}
h(i,j,k) =  T_1(i) + T_2(j-1) + T_3(k-2);
\end{align}
and from this the generalization to higher dimensions is straightforward. The formulas for ``higher triangular numbers'', i.e., those defined by
\begin{align}
T_k(n) &= \sum_{i=1}^n T_{k-1}(n)
\end{align}
for $k > 1$ can be determined inductively.

The explicit formula for 3-way interactions, with zero-based indexing, is 
\begin{align}
h(i, j, k) &= 1 + (i-1) + \frac{(j-1)j}{2} + \\
& \frac{(k-2)^3 + 3(k-2)^2 + 2(k-2)}{6}. 
\end{align}

\section{Final CSR Expansion Algorithm}
With the mapping from columns of $\bm{A}$ to a column of $\bm{A}^k$, we can now write the final form of the innermost loop of the algorithm from \ref{sec:final-algo}.
Let the mapping for $k=2$ be denoted $h^2$.
Then the innermost loop becomes:

\begin{codebox}
\footnotesize
    \zi         \For $c_2 \gets c_1 \To |\bm{c}_i|-1$ \Do
    \zi             $j_0 \gets \bm{c}_{c_0}$
    \zi             $j_1 \gets \bm{c}_{c_1}$
    \zi             $c_p \gets h^2(j_0, j_1)$
    \zi             $\bm{d}^k_{n} \gets \bm{d}_{c_0} \cdot \bm{d}_{c_1}$
    \zi             $\bm{c}^k_{n} = c_p$
    \zi             $n \gets n + 1$
                \End
\end{codebox}

The algorithm can be generalized to higher degrees by simply adding more nested loops, using higher order mappings, modifying the output dimensionality, and adjusting the counting of nonzero polynomial features in line \ref{li:row_nnz_count}.
%For interaction features, the interaction mappings can be used in lieu of the polynomial mappings with the additional change of the output dimensionality and the number of nonzero features in each row (combinations without repetition instead of with).

\section{Time Complexity}
\subsection{Analytical}

Calculating $k$-degree polynomial features via our method for a vector of dimensionality $D$ and density $d$ requires $\binom{dD}{k}$ (with repetition) products.
The complexity of the algorithm, for fixed $k \ll dD$, is therefore
\begin{align}
& O\left(\binom{dD+k-1}{k}\right) = O\left(\frac{(dD+k-1)!}{k!(dD-1)!}\right)\\
& = O\left(\frac{(dD+k-1)(dD+k-2) \dots (dD)}{k!}\right)\\
& = O\left((dD+k-1)(dD+k-2) \dots (dD)\right)\\
& = O\left(d^kD^k\right)
\end{align}

For a matrix of size $N \times D$, the complexity is therefore $O\left(Nd^kD^k\right)$.
The dense algorithm does not leverage sparsity, and its complexity is $O\left(ND^k\right)$.
Since $0 \le d \le 1$, the sparse algorithm scales polynomially with the degree of the polynomial expansion.

\subsection{Empirical}


\section{Conclusion}
We have developed an algorithm for performing polynomial feature expansions on CSR matrices that scales polynomially with respect to the density of the matrix.
The areas within machine learning that this work touches are not en vogue, but they are workhorses of industry, and every improvement in core representations has an impact across a broad range of applications. 
%This improvement could therefore spare the burning of much fossil fuel.

\bibliography{sparse_poly_icml}
\bibliographystyle{icml2017}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
