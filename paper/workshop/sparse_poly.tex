\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_workshop,times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}


\title{Efficient Calculation of Polynomial Features on Sparse Matrices}


\author{Andrew Nystrom \\
Savvysherpa Inc.\\
6200 Shingle Creek Pkwy \\
Suite 400 \\
Minneapolis, MN 55430, USA \\
\texttt{awnystrom@gmail.com} \\
\And
John Hughes \\
Department of Computer Science \\
Brown University \\
Providence, RI \\
\texttt{jfh@cs.brown.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
We provide an algorithm for polynomial feature expansion that operates directly on a sparse matrix.
For a vector of dimension $D$ and density $d$, the algorithm has time and space complexity $O(d^kD^k)$ where $k$ is the polynomial order.
\end{abstract}

% this is a test-change for Spike to see whether Git seems to work for him. Ignore. 

\section{Introduction}

Polynomial feature expansion has long been used in statistics to approximate nonlinear functions~\cite{gergonne1974application, smith1918standard}.
Despite this, we are unaware of any efforts to optimize calculating them.
Here we provide an algorithm for calculating polynomial features for a vector of dimension $D$ and density $d$ with time and space complexity $O(d^kD^k)$ where $k$ is the polynomial order, and $0 \le d \le 1$ is the fraction of elements that are nonzero.
The standard algorithm has time and space complexity $O(D^k)$, so the added factor of $d^k$ represents a significant complexity reduction.
The algorithm avoids densification of the vector, i.e. the vector remains in compressed sparse row form, so the space complexity is also $O(d^kD^k)$ as opposed to $O(D^k)$. 

\section{Algorithm}
In the naive computation of polynomial features for a vector $\vec{x}$, we create a new feature for each product (with repetition) of $k$ features in $\vec{x}$ (or without repetition, for ``interaction features'').
This ignores data sparsity and will yield a product of zero any time one of the features involved in the product is zero.
In a sparse matrix, such zero-products are common.
%If $\vec{x}$ has density $d$, the fraction of nonzero elements of the $k$-order expansion will be $\frac{\left(\binom{dD+k-1}{k}\right)}{\left(\binom{D+k-1}{k}\right)} = \frac{[(dD+k-1)(dD+k-2) \dots (dD+k)](D-1)!}{(dD-1)!} = $
If we store vectors in a sparse matrix format, these zero-products need not be computed or stored. 

The main idea behind our algorithm is to leverage sparsity by only computing products that do not involve zeros.
In a compressed sparse row matrix, the columns containing nonzero data are the only columns that are stored.
We can therefore iterate over products of combinations with repetition of order $k$ of \emph{only these columns} for each row to calculate $k$-degree polynomial features.

While the idea is straightforward, there is yet an unaddressed challenge:
Given a multiset of column indices whose corresponding nonzero components were multiplied to produce a polynomial feature, where in the augmented polynomial vector does the result of the product belong?
To address this, we give a bijective mapping from the set of possible column index combinations-with-repetition of order $k$ onto the column index space of the polynomial feature matrix. Thus the map has the form

\begin{equation}
(i_0, i_1, \dots, i_{k-1}) \rightarrowtail \hspace{-1.9ex} \twoheadrightarrow p_{i_0i_1 \dots i_{k-1}} \in \{0,1,\dots,\binom{D}{k}\} 
\end{equation}
such that $ 0 \le i_0 \le i_1 \le \dots \le i_{k-1} < D$
where $(i_0, i_1, \dots, i_{k-1})$ are column indicies of a row vector $\vec{x}$ of an $N \times D$ input matrix, and $p_{i_0i_1 \dots i_{k-1}}$ is a column index into the polynomial expansion vector for $\vec{x}$ where the product of elements corresponding to indices $i_0, i_1, \dots, i_{k-1}$ will be stored.
%and $\left(\binom{D}{k}\right)$ is the number of combinations with repetition of size $k$ drawn from $D$ objects.
%In general, $\left(\binom{n}{k}\right) = \binom{n+k-1}{n-1} = \binom{n+k-1}{k}$, as given by \cite{stanley1986enumerative}.

\subsection{Construction of Mappings}

We seek a map from matrix indices $(i, j)$ (with $i < j$ and $0 \le i < D$) to numbers $f(i, j)$ with $0 \le f(i, j) < \frac{D(D-1)}{2}$, one that follows the pattern indicated by 
\begin{align}
\begin{bmatrix}
x & 0 & 1 & 3 \\
x & x & 2 & 4 \\
x & x & x & 5 \\
x & x & x & x
\end{bmatrix}
\label{eq:4x4mat}
\end{align}
where the entry in row $i$, column $j$, displays the value $f(i, j)$. 

To simplify slightly, we introduce a notation for the $n$th triangular number, 
\begin{align}
T_2(n) = \frac{n(n+1)}{2}
\end{align}
\noindent
The subscript $2$ indicates that these are triangles in two dimensions; we'll use $T_3(n)$ to indicate the $n$th tetrahedral number, and so on for higher dimensions. 

Observe that in Equation~\ref{eq:4x4mat}, each entry in column $j$ (for $j > 0$) lies in the range
\begin{align}
T_2(j-1) &\le e < T_2(j).
\end{align}
\noindent
And in fact, the entry in the $i$th row of that column is just $i + T_2(j-1)$. Thus we have
\begin{align}
f(i, j) 
&= i + T_2(j-1)\\
&= i + \frac{(j-1)j}{2}\\
&=  \frac{2i + j^2-j}{2}.
\end{align}

For instance, in column $j = 2$ in our example (the \emph{third} column), the entries range from $1$ to $2$, while $T_2(j-1) = T_2(1) = 1$ and $T_2(j) = T_2(2) = 3$, and the entry in column $j = 2$, row $i = 1$ is 
$i + T_2(j-1) = 1 + 1 = 2$. 

\subsubsection{Other indices}
With one-based indexing in both the domain and codomain, the formula above becomes
\begin{align}
f_1(i, j) &= 1+ f(i-1, j-1) \\
&= 1+ f(i-1, j-1) \\
& = \frac{2 + 2(i-1) + (j-1)^2-(j-1)}{2}\\
& = \frac{2i + j^2 - 3j + 2}{2}
\end{align}

\subsubsection{Polynomial Features}
For polynommial features, we seek a map from matrix indices $(i, j)$ (with $i \le j$ and $0 \le i < D$) to numbers $g(i, j)$ with $0 \le f(i, j) < \frac{D(D+1)}{2}$, one that follows the pattern indicated by 
\begin{align}
\begin{bmatrix}
 0 & 1 & 3 & 6 \\
 x & 2 & 4 & 7\\
 x & x & 5 & 8 \\
 x & x & x & 9
\end{bmatrix}
\label{eq:4x4mat-poly}
\end{align}
i.e., essentially the same task as before, except that the diagonal is included. One can regard all but the last column of entries in Equation~\ref{eq:4x4mat-poly} as corresponding to the entries in Equation~\ref{eq:4x4mat}, but shifted to the left. Thus the formula for $g(i, j)$ is simply the formula for $f$, shifted by 1, i.e., 
\begin{align}
g(i, j) &= f(i, j+1)  \\
&=  \frac{2i + (j+1)^2-(j+1)}{2}\\
&=  \frac{2i + j^2 + j + 1)}{2}.
\end{align}
Alternatively, we can write this as
\begin{align}
g(i, j) &= i + T_2(j),
\end{align}
\noindent
and get the same result. 


\subsubsection{Higher dimensions}
To handle three-way interactions, we need to map triples of indices in a 3-index array to a flat list, and similarly for higher-order interactions. 

For three indices, $i,j,k$, with $i < j < k$ and $0 \le i,j,k < D$, we have a similar recurrence. Calling the mapping $h$, we have 
\begin{align}
h(i,j,k) = i + T_2(j-1) + T_3(k-2);
\end{align}
if we define $T_1(i) = i$, then this has the very regular form
\begin{align}
h(i,j,k) =  T_1(i) + T_2(j-1) + T_3(k-2);
\end{align}
and from this, the generalization to higher dimensions is straightforward. The formulas for ``higher triangular numbers'', i.e., those defined by
\begin{align}
T_k(n) &= \sum_{i=1}^n T_{k-1}(n)
\end{align}
for $k > 1$ can be determined inductively. For $k = 3$, the result is 
\begin{align}
T_3(n) &= \sum_{i=1}^n T_{2}(n)\\
&= \frac{n^3 + 3n^2 + 2n}{6},
\end{align}
so that the formula for 3-way interactions, with zero-based indexing, becomes 
\begin{align}
h(i, j, k) &= 1 + (i-1) + \frac{(j-1)j}{2} + \\
& \frac{(k-2)^3 + 3(k-2)^2 + 2(k-2)}{6}. 
\end{align}
\subsubsection{Higher-dimension polynomial features}
For the case where we include the diagonal in higher dimensions, we must shift $j$ by $1$, $k$ by $2$, and so on, and the formula becomes
\begin{align}
\ell(i,j,k) =  T_1(i) + T_2(j) + T_3(k),
\end{align}
with analogous formulas for higher degree polynomial interactions. 

\section{Analysis}
\subsection{Analytical}

Calculating $k$-degree polynomial features via our method for a vector of dimensionality $D$ and density $d$ requires $dD$ choose $k$ (with repetition) products.
The big O of the algorithm is therefore given by
\begin{align}
O\left(\binom{dD+k-1}{k}\right) & = O\left(\frac{(dD+k-1)!}{k!(dD-1)!}\right)\\
& = O\left(\frac{(dD+k-1)(dD+k-2) \dots (dD)}{k!}\right)\\
& = O\left((dD+k-1)(dD+k-2) \dots (dD)\right) \mbox{ for } k \ll dD\\
& = O\left(d^kD^k\right)
\end{align}

\subsection{Empirical}
\section{Conclusion}


\bibliography{sparse_poly}
\bibliographystyle{iclr2016_workshop}

\end{document}
