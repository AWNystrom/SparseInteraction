\documentclass[11pt]{article}

\usepackage{array}
\usepackage{clrscode3e}
\usepackage{amsmath}
\usepackage{kbordermatrix}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\title{Efficient Calculation of Interaction Features on Sparse Matrices}
\author{Andrew Nystrom}
\date{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
FILL THIS IN
\end{abstract}

\section{Introduction}

Introduction
Interaction features are a way of capturing correlations between features in a machine 
learning setting. A feature vector $\vec{x}$ of dimensionality $D$ has second degree interaction features 
$\{x_i \cdot x_j : i, j \in \{0,1,..., D-1\} \land i < j\}$, 
so a $D$ dimensional vector has $\binom{D}{2} = \frac{D^2-D}{2}$ second degree interaction features. A naive
approach to calculating these features is to simply iterate through the combinations of the column indices.
For a sparse vector, many of the resulting interaction features would be zero, and could therefore be ignored.
This work describes a method to efficiently calculate second degree interaction features for a sparse matrix 
that has time and space complexities that decrease quadratically with the density of the input matrix with respect to the naive approach.

$\{x_i \cdot x_j : i, j \in \{0,1,..., D-1\} \land i \le j\}$  \\
\\
$\{(i,j) : i, j \in \{0,1,..., D-1\} \land i \le j\} \to \{0, 1, ..., \frac{D^2+D}{2}-1\}$ \\

$i$
$j$

$\vec{x}$

\begin{codebox}
\Procname{$\proc{Dense Polynomials}(A)$}
    \zi $N \gets$ row count of $A$
    \zi $D \gets$ column count of $A$
    \zi $B$ $\gets$ Matrix of size $N \times \frac{D^2+D}{2}$
    \zi \For $\id{row}$ in $A$ \Do
    \zi     $k \gets 0$
    \zi     \For $\i \gets 0 \To D$ \Do
    \zi         \For $\j \gets i \To D$ \Do
    \zi             $B[i, k] \gets \id{row}[i] \cdot \id{row}[j]$
    \zi             $k \gets k + 1$
                \End
            \End
       	\End
\end{codebox}

$[a, b, c, d, e]$

$[ab, ac, ad, ae, bc, bd, be, cd, ce, de]$


\renewcommand{\kbldelim}{(}% Left delimiter
\renewcommand{\kbrdelim}{)}% Right delimiter
\[
  \kbordermatrix{
        & x_0 & x_1 & x_2 & x_3 & x_4 \\
    x_0 & 0 & 1 & 2 & 3 & 4\\
    x_1 & - & 5 & 6 & 7 & 8\\
    x_2 & - & - & 9 & 10 & 11\\
    x_3 & - & - & - & 12 & 13\\
    x_4 & - & - & - & - & 14\\
  }
\]


$\text{polynomial-index}(i, j | D) = \frac{2Di-i^2+2j-3i-2}{2}+i+1$

%Consider the following matrix:

%\[ \left( \begin{array}{cccc}
%3 & 0 & 0 & 3 \\
%0 & 4 & 2 & 0 \\
%1 & 2 & 0 & 3 \end{array} \right)\]
\[ \left( \begin{array}{ccccc}
A & b & c & d \end{array} \right)\]

%\[ \left( \begin{array}{cccccccccc}
%1 & 2 & 0 & 3 & 0 \end{array} \right)\]


%If each row is an instance vector, then the interaction feature matrix is

%\[ \left( \begin{array}{cccccc}
%0 & 0 & 9 & 0 & 0 & 0 \\
%0 & 0 & 0 & 8 & 0 & 0 \\
%2 & 0 & 3 & 0 & 6 & 0 \end{array} \right)\]

%Note that there are $\binom{D}{2} = \frac{D^2-D}{2}$ columns in the interaction matrix, which is D choose 2, 
%since we generate products of all combinations of 2 features in the original matrix. 
%Which column corresponds with which product pair is not important so long as itâ€™s 
%consistent. In this example, each column in the interaction matrix corresponded to the 
%following product pairs: $(1, 2),  (1, 3),  (1, 4),  (2, 3), ( 2, 4), (3, 4)$.

%Notice that the interaction matrix contains many zero entries. This is of course because 
%the original matrix contained zero entries, so the interaction features, which are 
%products of pairs of features, contain many zeros. This means that the only products that 
%need to actually be calculated or those for which both features in the combination are 
%nonzero. If the original matrix is sparse and represented in a sparse matrix format 
%(e.g. compressed sparse row), a list of nonzero column indices are stored for each row 
%and are easily retrievable in $O(1)$ time. Interaction features can be generated from this 
%list via the following method.

\section{Approach}
Let the list of nonzero columns for a given row $\vec{x}$ be denoted by $N_{zc}$. The nonzero second degree 
interaction features are simply the products of all combinations of two elements whose 
columns are in $N_{zc}$. However, to properly place an interaction feature into the correct column, a mapping from the column 
index pairs of $N_{zc}$ into the columns of the interaction matrix is needed. The mapping is 
from the space (a, b) where a,b are in $1,2,..., D$ onto the space $1,2,..., \frac{D^2-D}{2}$. This 
is isomorphic to mapping the coordinates of the upper triangle of a matrix onto a flat 
list. The following is a proof by construction for such a mapping.

INSERT JOHN'S PROOF HERE

With this mapping, an algorithm for generating second degree interaction features on a 
matrix $A$ can be formulated as follows:

\begin{codebox}
\Procname{$\proc{Sparse Polynomials}(A)$}
    \zi $\func{map}(a, b) = \frac{2Da-a^2+2b-3a-2}{2}+a+1$
    \zi $N \gets$ row count of $A$
    \zi $D \gets$ column count of $A$
    \zi $B$ $\gets$ Compressed Sparse Row Matrix of size $N \times \frac{D^2+D}{2}$
    \zi \For $\id{row}$ in $A$ \Do
    \zi     $N_{zc} \gets$ nonzero columns of $row$
    \zi     \For $i \gets 0 \To |N_{zc}|$ \Do
    \zi         \For $j \gets i \To |N_{zc}|$ \Do
    \zi             $k \gets \func{map}(i, j)$
    \zi             $r \gets$ index of $\id{row}$
    \zi             $B[r, k] \gets \id{row}[i] \cdot \id{row}[j]$
                \End
            \End
       	\End
\end{codebox}

\section{Complexity Analysis}
Assume that A is a matrix with sparsity $0 < d < 1$, $N$ rows, and $D$ columns. Finding 
interaction features with the proposed algorithm has time and space complexity 
$\func{O}(d N D^2)$, 
whereas a naive approach of using non-sparse matrices and multiplying all column 
combinations has time and space complexity $\func{O}(N D^2)$. The algorithm is therefore an 
improvement by a factor of the density factor of $A$.

This can represent a large gain in speed and time. For example, the 20 Newsgroups dataset 
has density $d$ of 0.12 when its unigrams are represented in a vector space model. This 
means the proposed approach would take less than $\frac{1}{8}$ time and memory.

The real benefit of this method is revealed when the average complexity is analysed. The 
number of interactions calculated for a given row are $\binom{|N_{zc}|}{2}$. If the matrix has 
density $d$, then on average, $N_{zc} = D d$, so the number of interaction features 
calculated in total is 

\begin{align*}
N \binom{d D}{2} &= N \frac{(Dd)!}{2!(Dd-2)!}\\
    \\
    &= N \frac{(D^2d^2-Dd)}{2}
\end{align*}

This means that the average complexity decreases quadratically with the density.

\section{Future Work}
The approach for generating second degree interaction features required a mapping from 
combinations of two to the space $1,2,...,\frac{D^2-D}{2}$, which is isomorphic to a mapping from 
the indices of an upper triangular matrix to the indices of a flat list of the same size. 
To generate third degree interaction features, a mapping from combinations of three 
$(a,b,c)$ to the space $1,2,...\frac{D^3-3D^2+2D}{6}$ (which is $\binom{D}{3}$), or the upper $3$-simplex of a tensor to a flat 
list of the same size $\frac{D^3-3D^2+2D}{6}$ would be required. In general, for interaction 
features of degree k, the upper $k$-simplex of a $k$-dimensional tensor must be mapped to the 
space $1,2,...\frac{D!}{k!(D-k)!}$. A similar approach for finding these mappings could be taken 
as the one used here for $k=2$. 

Motivation for deriving mapping functions for higher orders
of interaction features is that the average complexity of generating degree $k$ interaction
features is $N \binom{Dd}{k}$, which decreases polynomially with respect to k compared to
generating the features naively.

    
\vskip 0.2in
\bibliography{sample}

\end{document}